{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ab Initio to PySpark Code Conversion\n",
    "\n",
    "This notebook implements an Azure OpenAI service-based code conversion agent that converts Ab Initio code to PySpark. The agent will:\n",
    "1. Read XFR files and schema layouts\n",
    "2. Generate PySpark transformation code\n",
    "3. Perform code review and optimization\n",
    "4. Output production-ready PySpark code\n",
    "\n",
    "# Package Installation and Setup\n",
    "First, let's install all the required packages for our code interpreter agent. We'll use pip to install them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Install required packages\n",
    "# !pip install openai python-dotenv pandas jupyter notebook ipykernel requests matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Required Libraries\n",
    "Import necessary libraries for working with Azure OpenAI, environment variables, and data processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import openai\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import pandas as pd\n",
    "import requests\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load environment variables from root directory\n",
    "root_env_path = Path(__file__).parent.parent / '.env'\n",
    "load_dotenv(root_env_path)\n",
    "\n",
    "print(f\"Loading .env file from: {root_env_path}\")\n",
    "print('All packages imported successfully!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from openai import AzureOpenAI\n",
    "import os\n",
    "# import dependencies\n",
    "\n",
    "import os\n",
    "from openai import AzureOpenAI\n",
    "from dotenv import load_dotenv\n",
    "import json\n",
    "import copy\n",
    "import textwrap\n",
    "\n",
    "# Using the same environment variables loaded above\n",
    "\n",
    "client = AzureOpenAI(  \n",
    "    api_version=os.getenv(\"AZURE_OPENAI_API_VERSION\"),\n",
    "    azure_endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT\"),  \n",
    "    api_key=os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    ")\n",
    "\n",
    "# class CaseStudy(BaseModel):\n",
    "#     case_study_task: str = Field(..., \n",
    "#         description=\"You are a data engineer with expertise in Ab Initio and PySpark. You will be provided with a case study task that requires you to analyze and process data using these technologies. Your goal is to provide a solution that meets the requirements outlined in the task description.\")\n",
    "#     case_study_solution: str = Field(..., \n",
    "#         description=\"The expected solution to the case study.\")\n",
    "\n",
    "\n",
    "def o3minicall(prompt, reasoning_effort, response_format=None):\n",
    "\n",
    "    system_message = \"\"\"\n",
    "    You are a **principal data engineer** who is fluent in both **Ab Initio XFR** and **PySpark** (Databricks).  \n",
    "    Your task is to translate a given XFR file into a production-ready PySpark solution.\n",
    "\n",
    "    ### 1. Deliverables  \n",
    "    | Item | Description |\n",
    "    |------|-------------|\n",
    "    | **A. Modular functions** | Reusable PySpark functions that replicate every XFR rule or sub-graph.<br>• Name each function after the business rule it implements.<br>• No hard-coded paths or secrets—parameterize where appropriate. |\n",
    "    | **B. Pipeline assembly** | A single `main()` (or notebook cell) that:<br>1. Reads **`input_schema`** (from the provided layout).<br>2. Sequentially applies the modular functions.<br>3. Selects / renames columns to match **`output_schema`**.<br>4. Writes the result (Parquet or table) ready for Databricks jobs. |\n",
    "    | **C. Step-by-step explanation** | For every function and pipeline stage, include a concise markdown comment explaining *what* it does and *why* (1-3 sentences).<br>Focus on business logic, joins, aggregations, date maths, and default rules. |\n",
    "\n",
    "    ### 2. Input Artifacts (available in variables)  \n",
    "    * `xfr_content` - full Ab Initio logic  \n",
    "    * `input_layout` - markdown layout file for the source schema  \n",
    "    * `output_layout` - markdown layout file for the target schema  \n",
    "\n",
    "    ### 3. Coding Guidelines  \n",
    "    * Use **PySpark 3.x DataFrame API** only (no RDDs).  \n",
    "    * Optimize for readability first; add `.cache()` only where beneficial.  \n",
    "    * Follow PEP-8 naming (e.g., `add_company_number`, `calculate_policy_term`).  \n",
    "    * Keep all string literals in a dedicated **`constants.py`** block (you may inline in the prompt for brevity).  \n",
    "    * Validate data types explicitly; cast to `StringType` where the XFR expects `hive_string_t`.\n",
    "\n",
    "    ### 4. Output Format  \n",
    "    Provide a **single markdown code block** containing:  \n",
    "\n",
    "    1. `import` statements and any constant dictionaries  \n",
    "    2. All modular function definitions  \n",
    "    3. The end-to-end pipeline assembly (`main()` or notebook cells)  \n",
    "    4. Inline markdown comments for explanations  \n",
    "\n",
    "    ---\n",
    "\n",
    "    **Begin converting now.**\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create the base parameters for the API call\n",
    "    params = {\n",
    "        \"model\": \"o3\",  # replace with the model deployment name\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"user\", \n",
    "                \"content\": system_message+\" \"+prompt\n",
    "            }\n",
    "        ],\n",
    "        \"reasoning_effort\": reasoning_effort\n",
    "    }\n",
    "    \n",
    "    # Only add response_format if it's provided\n",
    "    if response_format is not None:\n",
    "        params[\"response_format\"] = response_format\n",
    "        \n",
    "    # Make the API call with the appropriate parameters\n",
    "    completion = client.chat.completions.create(**params)\n",
    "    \n",
    "\n",
    "    # Comment out since 'event' is not defined\n",
    "    # print(event)\n",
    "    \n",
    "    # print(completion.model_dump_json(indent=2))\n",
    "    return completion\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define inputs\n",
    "\n",
    "## Define Example Case Study\n",
    "\n",
    "Provide an example case study showing the conversion of ASC_VIP_Premium Ab Initio workflow to PySpark, including:\n",
    "- Input/Output schema definitions\n",
    "- Transformation functions\n",
    "- Pipeline assembly\n",
    "- Data validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_case_studies=\"\"\"\n",
    "### START EXAMPLE CASE STUDY\n",
    "\n",
    "**Converting the ASC_VIP_Premium Ab Initio Workflow to PySpark**\n",
    "\n",
    "Below is a complete walkthrough showing how to translate the Ab Initio *ASC_VIP_Premium* workflow (defined in a `.xfr` transform) into an equivalent, modular PySpark pipeline.\n",
    "The pipeline reads source data, applies every business rule, and writes an output DataFrame that matches the required layout.\n",
    "\n",
    "---\n",
    "\n",
    "## 1 Input Data Schema and Loading\n",
    "\n",
    "We start by defining the schema described in `simple_input_layout.txt`.\n",
    "All original fields are `hive_string_t`, so we map them to `StringType` in Spark:\n",
    "\n",
    "```python\n",
    "from pyspark.sql.types import StructType, StructField, StringType\n",
    "\n",
    "input_schema = StructType([\n",
    "    StructField(\"messageid\",             StringType(), True),\n",
    "    StructField(\"agreementid\",           StringType(), True),\n",
    "    StructField(\"systementcd\",           StringType(), True),\n",
    "    StructField(\"sourcesystementcd\",     StringType(), True),\n",
    "    StructField(\"transactionuserid\",     StringType(), True),\n",
    "    StructField(\"transactioneffdttime\",  StringType(), True),\n",
    "    StructField(\"transactiontypeentcd\",  StringType(), True),\n",
    "    StructField(\"transactionprocesseddttimestr\", StringType(), True),\n",
    "    StructField(\"policynbr\",             StringType(), True),\n",
    "    StructField(\"policyversionnbr\",      StringType(), True),\n",
    "    StructField(\"premiumTypeEntCd\",      StringType(), True),\n",
    "    StructField(\"contracttermexpdttimestr\", StringType(), True),\n",
    "    StructField(\"contracttermlengthcnt\", StringType(), True),\n",
    "    StructField(\"contracttermeffdttime\", StringType(), True),\n",
    "    StructField(\"downPaymentAmtStr\",     StringType(), True),\n",
    "    StructField(\"downPaymentPct\",        StringType(), True),\n",
    "    StructField(\"downPaymentPctStr\",     StringType(), True),\n",
    "    StructField(\"accountingcompanyentcd\",StringType(), True),\n",
    "    StructField(\"stateproventcd\",        StringType(), True),\n",
    "    StructField(\"coverageentcd\",         StringType(), True),\n",
    "    StructField(\"coverageeffdttimestr\",  StringType(), True),\n",
    "    StructField(\"postalcode\",            StringType(), True),\n",
    "    StructField(\"fiscalperiod\",          StringType(), True),\n",
    "    StructField(\"peroccurrencelimitamtstr\", StringType(), True),\n",
    "    StructField(\"totalpremiumamtstr\",    StringType(), True),\n",
    "    StructField(\"fipscountyentcd\",       StringType(), True),\n",
    "    StructField(\"allstatecountycd\",      StringType(), True),\n",
    "    StructField(\"cityname\",              StringType(), True),\n",
    "    StructField(\"policystatusentcd\",     StringType(), True),\n",
    "    StructField(\"policyterminatedentcd\", StringType(), True),\n",
    "    StructField(\"netchangeamt\",          StringType(), True),\n",
    "    StructField(\"recordtype\",            StringType(), True),\n",
    "    StructField(\"batchid\",               StringType(), True),\n",
    "    StructField(\"rawfilename\",           StringType(), True),\n",
    "    StructField(\"recordsequenceid\",      StringType(), True),\n",
    "    StructField(\"accountingyear\",        StringType(), True),\n",
    "    StructField(\"accountingmonth\",       StringType(), True)\n",
    "])\n",
    "\n",
    "df_input = (\n",
    "    spark.read.format(\"csv\")\n",
    "    .schema(input_schema)\n",
    "    .option(\"header\", \"false\")\n",
    "    .load(\"/path/to/ASC_VIP_Premium_input_data\")\n",
    ")\n",
    "```\n",
    "\n",
    "*(In production you may load from Hive or Parquet; keeping everything as strings preserves fidelity with the Ab Initio types.)*\n",
    "\n",
    "---\n",
    "\n",
    "## 2 Transformation Functions\n",
    "\n",
    "### 2.1 Company Number Mapping\n",
    "\n",
    "```python\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "COMPANY_MAP = {\n",
    "    10:  \"0031\",  27:  \"1585\",  20:  \"0025\",  21:  \"0776\",  22:  \"0002\",\n",
    "    60:  \"0520\",  63:  \"0467\",  64:  \"0067\",  65:  \"0458\",  68:  \"1305\",\n",
    "    70:  \"0420\",  85:  \"0517\",  95:  \"0522\",  330: \"1481\",  270: \"0192\",\n",
    "    339: \"1437\",  365: \"1534\",  382: \"1546\",  383: \"1545\",  357: \"1562\",\n",
    "    359: \"1564\",  360: \"1560\",  361: \"1561\",  367: \"1563\",  387: \"2206\",\n",
    "    386: \"2207\"\n",
    "}\n",
    "DEFAULT_COMPANY = \"1481\"\n",
    "\n",
    "def add_companynumber(df):\n",
    "    return df.withColumn(\n",
    "        \"companynumber\",\n",
    "        F.coalesce(\n",
    "            F.expr(\n",
    "                \"CASE CAST(accountingcompanyentcd AS INT) \" +\n",
    "                \" \".join([f\"WHEN {k} THEN '{v}'\" for k, v in COMPANY_MAP.items()]) +\n",
    "                \" END\"\n",
    "            ),\n",
    "            F.lit(DEFAULT_COMPANY)\n",
    "        )\n",
    "    )\n",
    "```\n",
    "\n",
    "### 2.2 Policy Term Calculation\n",
    "\n",
    "```python\n",
    "from pyspark.sql.functions import year, month, to_timestamp, lit, lpad\n",
    "\n",
    "def add_policyterm(df):\n",
    "    df = (\n",
    "        df.withColumn(\"trans_eff_dt\", to_timestamp(\"transactioneffdttime\"))\n",
    "          .withColumn(\"term_exp_dt\",  to_timestamp(\"contracttermexpdttimestr\"))\n",
    "          .withColumn(\"term_eff_dt\",  to_timestamp(\"contracttermeffdttime\"))\n",
    "          .withColumn(\n",
    "              \"month_diff\",\n",
    "              (year(\"term_exp_dt\") - year(\"trans_eff_dt\")) * 12 +\n",
    "              (month(\"term_exp_dt\") - month(\"trans_eff_dt\"))\n",
    "          )\n",
    "    )\n",
    "\n",
    "    cond_newbiz          = F.col(\"transactiontypeentcd\") == lit(\"0001\")\n",
    "    cond_endorse_nonzero = F.col(\"transactiontypeentcd\").isin(\"0002\", \"0010\") & (F.col(\"month_diff\") != 0)\n",
    "    cond_endorse_zero    = F.col(\"transactiontypeentcd\").isin(\"0002\", \"0010\") & (F.col(\"month_diff\") == 0)\n",
    "    cond_cancel          = F.col(\"transactiontypeentcd\").isin(\"0004\", \"0012\")\n",
    "    cond_cancel_sameDay  = cond_cancel & (F.to_date(\"trans_eff_dt\") == F.to_date(\"term_eff_dt\"))\n",
    "    cond_cancel_ge12     = cond_cancel & (F.col(\"month_diff\") >= 12)\n",
    "    cond_cancel_le1      = cond_cancel & (F.col(\"month_diff\") <= 1)\n",
    "\n",
    "    df = df.withColumn(\n",
    "        \"policyterm\",\n",
    "        F.when(cond_newbiz, lpad(\"contracttermlengthcnt\", 2, \"0\"))\n",
    "         .when(cond_endorse_nonzero, lpad(F.col(\"month_diff\").cast(\"string\"), 2, \"0\"))\n",
    "         .when(cond_endorse_zero,    lit(\"01\"))\n",
    "         .when(cond_cancel_sameDay,  lpad(\"contracttermlengthcnt\", 2, \"0\"))\n",
    "         .when(cond_cancel_ge12,     lit(\"11\"))\n",
    "         .when(cond_cancel_le1,      lit(\"01\"))\n",
    "         .when(cond_cancel,          lpad(F.col(\"month_diff\").cast(\"string\"), 2, \"0\"))\n",
    "         .otherwise(lpad(\"contracttermlengthcnt\", 2, \"0\"))\n",
    "    )\n",
    "\n",
    "    return df.drop(\"trans_eff_dt\", \"term_exp_dt\", \"term_eff_dt\", \"month_diff\")\n",
    "```\n",
    "\n",
    "### 2.3 Policy Effective Year\n",
    "\n",
    "```python\n",
    "def add_policyeffectiveyear(df):\n",
    "    df = (\n",
    "        df.withColumn(\"trans_eff_date\",      to_timestamp(\"transactioneffdttime\"))\n",
    "          .withColumn(\"policy_term_eff_date\", to_timestamp(\"contracttermeffdttime\"))\n",
    "    )\n",
    "\n",
    "    cond_endorse    = F.col(\"transactiontypeentcd\") == lit(\"0002\")\n",
    "    cond_cancel_mid = (\n",
    "        F.col(\"transactiontypeentcd\").isin(\"0004\", \"0012\") &\n",
    "        (F.to_date(\"trans_eff_date\") != F.to_date(\"policy_term_eff_date\"))\n",
    "    )\n",
    "\n",
    "    df = df.withColumn(\n",
    "        \"policyeffectiveyear\",\n",
    "        F.when(cond_endorse | cond_cancel_mid, F.date_format(\"trans_eff_date\", \"yyyy\"))\n",
    "         .otherwise(F.date_format(\"policy_term_eff_date\", \"yyyy\"))\n",
    "    )\n",
    "\n",
    "    return df.drop(\"trans_eff_date\", \"policy_term_eff_date\")\n",
    "```\n",
    "\n",
    "### 2.4 Classification Code Mapping\n",
    "\n",
    "```python\n",
    "def add_classificationcode(df):\n",
    "    cov   = F.col(\"coverageentcd\").cast(\"int\")\n",
    "    state = F.col(\"stateproventcd\")\n",
    "\n",
    "    df = df.withColumn(\n",
    "        \"classificationcode\",\n",
    "        F.when((state != \"NY\") & (cov == 317), \"722000\")\n",
    "         .when((state != \"NY\") & (cov == 318), \"709700\")\n",
    "         .when((state != \"NY\") & (cov == 319), \"721000\")\n",
    "         .when((state != \"NY\") & (cov == 320), \"751300\")\n",
    "         .when((state != \"NY\") & (cov == 321), \"751500\")\n",
    "         .when((state != \"NY\") & (cov == 322), \"799900\")\n",
    "         .when((state != \"NY\") & (cov == 323), \"714400\")\n",
    "         .when((state != \"NY\") & (cov == 324), \"714200\")\n",
    "         .when((state != \"NY\") & (cov == 325), \"717700\")\n",
    "         .when((state == \"NY\") & (cov == 317), \"703200\")\n",
    "         .when((state == \"NY\") & (cov == 319), \"705100\")\n",
    "         .otherwise(\"\")\n",
    "    )\n",
    "    return df\n",
    "```\n",
    "\n",
    "### 2.5 Miscellaneous Constant / Simple Fields\n",
    "\n",
    "```python\n",
    "from datetime import datetime\n",
    "\n",
    "def add_misc_fields(df, filing_version=\"1.0\"):\n",
    "    run_ts = datetime.now().strftime(\"%Y-%m-%d%H:%M:%S\")\n",
    "\n",
    "    return (\n",
    "        df\n",
    "        .withColumn(\"lineofbusinesscode\",  F.lit(\"06\"))\n",
    "        .withColumn(\"statecode\",           F.col(\"stateproventcd\"))\n",
    "        .withColumn(\n",
    "            \"callyear\",\n",
    "            F.when(F.col(\"accountingyear\").cast(\"int\").isNotNull(),\n",
    "                   (F.col(\"accountingyear\").cast(\"int\") + 1).cast(\"string\"))\n",
    "             .otherwise(F.lit(\"\"))\n",
    "        )\n",
    "        .withColumn(\"experienceperiodyear\", F.lit(\"0000\"))\n",
    "        .withColumn(\"experienceperiodmonth\",F.lit(\"00\"))\n",
    "        .withColumn(\"experienceperiodday\",  F.lit(\"00\"))\n",
    "        .withColumn(\"typeoflosscode\",       F.lit(\"00\"))\n",
    "        .withColumn(\"annualstatementlobcd\", F.lit(\"091\"))\n",
    "        .withColumn(\"policyidentification\", F.lit(\"10\"))\n",
    "        .withColumn(\"claimantidentifier\",   F.lit(\"000\"))\n",
    "        .withColumn(\"claimidentifier\",      F.lit(\"0\"*15))\n",
    "        .withColumn(\"writtenpremium\",\n",
    "                    F.when(F.length(\"netchangeamt\") > 0, F.col(\"netchangeamt\"))\n",
    "                     .otherwise(F.lit(\"0\")))\n",
    "        .withColumn(\"paidlosses\",           F.lit(\"0\"*12))\n",
    "        .withColumn(\"paidclaims\",           F.lit(\"0\"*12))\n",
    "        .withColumn(\"outstandinglosses\",    F.lit(\"0\"*12))\n",
    "        .withColumn(\"outstandingclaims\",    F.lit(\"0\"*12))\n",
    "        .withColumn(\"filingtype\",           F.lit(\"PREMIUM\"))\n",
    "        .withColumn(\"filingruntimestamp\",   F.lit(run_ts))\n",
    "        .withColumn(\"filingsversion\",       F.lit(filing_version))\n",
    "    )\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 3 End-to-End Pipeline Assembly\n",
    "\n",
    "```python\n",
    "df_transformed = df_input\n",
    "df_transformed = add_companynumber(df_transformed)\n",
    "df_transformed = add_policyterm(df_transformed)\n",
    "df_transformed = add_policyeffectiveyear(df_transformed)\n",
    "df_transformed = add_classificationcode(df_transformed)\n",
    "df_transformed = add_misc_fields(df_transformed, filing_version=\"1.0\")\n",
    "```\n",
    "\n",
    "### Select Final Output Columns\n",
    "\n",
    "```python\n",
    "df_output = df_transformed.select(\n",
    "    \"messageid\", \"agreementid\", \"companynumber\", \"lineofbusinesscode\", \"statecode\",\n",
    "    \"callyear\", \"experienceperiodyear\", \"experienceperiodmonth\", \"experienceperiodday\",\n",
    "    \"classificationcode\", \"typeoflosscode\", \"policyeffectiveyear\",\n",
    "    \"annualstatementlobcd\", \"policyidentification\", \"policyterm\",\n",
    "    \"claimantidentifier\", \"claimidentifier\", \"writtenpremium\",\n",
    "    \"paidlosses\", \"paidclaims\", \"outstandinglosses\", \"outstandingclaims\",\n",
    "    \"policynbr\", \"recordsequenceid\", \"policyversionnbr\", \"coverageentcd\",\n",
    "    \"transactiontypeentcd\", \"recordtype\", \"filingtype\", \"filingruntimestamp\",\n",
    "    \"filingsversion\", \"accountingyear\", \"accountingmonth\"\n",
    ")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 4 Writing the Output\n",
    "\n",
    "```python\n",
    "# As Parquet\n",
    "df_output.write.mode(\"overwrite\").parquet(\"/path/to/output/ASC_VIP_Premium_transformed.parquet\")\n",
    "\n",
    "# Or as a Hive table\n",
    "df_output.write.mode(\"overwrite\").saveAsTable(\"prod.asc_vip_premium_output\")\n",
    "```\n",
    "\n",
    "The resulting Parquet files (or Hive table) exactly match the required output schema while implementing every business rule from the original Ab Initio workflow in PySpark.\n",
    "\n",
    "### END EXAMPLE CASE STUDY\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the files from Complex directory\n",
    "import os\n",
    "\n",
    "# Define absolute paths\n",
    "base_dir = r'c:\\Users\\kapildhanger\\OneDrive - Microsoft\\Microsoft_Kapil\\AzureCustomers\\AllState\\aicodeconversion\\Complex'\n",
    "xfr_file = os.path.join(base_dir, 'PersonalAuto_Premium_iFiling.xfr')\n",
    "input_layout = os.path.join(base_dir, 'complex_input_layout.txt')\n",
    "output_layout = os.path.join(base_dir, 'complex_output_layout.txt')\n",
    "\n",
    "# Read files\n",
    "try:\n",
    "    with open(xfr_file, 'r') as f:\n",
    "        xfr_content = f.read()\n",
    "        \n",
    "    with open(input_layout, 'r') as f:\n",
    "        input_layout_content = f.read()\n",
    "        \n",
    "    with open(output_layout, 'r') as f:\n",
    "        output_layout_content = f.read()\n",
    "        \n",
    "    print(\"Files read successfully!\")\n",
    "    print(\"\\nFiles content lengths:\")\n",
    "    print(f\"XFR file: {len(xfr_content)} characters\")\n",
    "    print(f\"Input layout: {len(input_layout_content)} characters\")\n",
    "    print(f\"Output layout: {len(output_layout_content)} characters\")\n",
    "except Exception as e:\n",
    "    print(f\"Error reading files: {e}\")\n",
    "scenario=f\"\"\"\n",
    "\n",
    "You are a data engineer with expertise in Ab Initio and PySpark.\n",
    "\n",
    "Convert the following Ab Initio XFR transformation logic into reusable PySpark functions. \n",
    "Convert this to modular PySpark code suitable for Databricks.\n",
    "Also create an end-to-end Spark DataFrame pipeline that reads input using the input layout, \n",
    "applies the logic, and outputs a DataFrame matching the output layout.\n",
    "\n",
    "The final result will be a complete PySpark workflow that reads \n",
    "the input data, applies all the business rules, and writes the output, tailored for execution in Databricks.\n",
    "Provide the explanation for each step.\n",
    "\n",
    "**Strict output rules**  \n",
    "• Return **one complete Python code block** and **nothing else** (no prose, no headings).  \n",
    "• All explanations must appear as inline comments inside that code block.  \n",
    "• The script must:  \n",
    "   1. Define reusable PySpark functions that replicate every XFR rule.  \n",
    "   2. Build an end-to-end DataFrame pipeline that:  \n",
    "      a. Reads data using the *Input Layout* schema.  \n",
    "      b. Applies all business-rule functions.  \n",
    "      c. Selects/renames columns to match the *Output Layout*.  \n",
    "      d. Writes the final DataFrame (Parquet or table) for Databricks jobs.  \n",
    "• No extra commentary outside the code fence.\n",
    "\n",
    "=== Input Layout ===\n",
    "{input_layout_content}\n",
    "\n",
    "=== Output Layout ===\n",
    "{output_layout_content}\n",
    "\n",
    "=== XFR Logic ===\n",
    "{xfr_content}\n",
    "\n",
    "=== Example ===\n",
    "Example case study is:\n",
    "{example_case_studies}\n",
    "\n",
    "================================================================\n",
    "Produce only the PySpark script:\n",
    "```python\n",
    "# (model inserts the complete Databricks-ready PySpark code with inline comments)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instructions for Ab Initio to PySpark conversion:\n",
    "# 1. Extract transformations and business rules from Ab Initio .xfr and HTML report\n",
    "# 2. Convert each sub-model into modular PySpark functions\n",
    "# 3. Create complete PySpark workflow for Databricks execution\n",
    "# 4. Provide PySpark code with explanations for each step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(scenario)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Develop the case study\n",
    "\n",
    "## Generate and Review Code\n",
    "\n",
    "Generate PySpark code from the input XFR file and perform a detailed code review to ensure:\n",
    "- Schema alignment\n",
    "- Performance optimization\n",
    "- Best practices compliance\n",
    "- Security and governance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_case_study=o3minicall(scenario,\"high\")\n",
    "print(output_case_study.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "code_review_prompt=f\"\"\"\n",
    "You are a principal Spark architect and code-review specialist.\n",
    "\n",
    "=============================  TASK 1 - DETAILED REVIEW  =============================\n",
    "**Cross-check the code against the supplied Input Layout and Output Layout.**  \n",
    "For every issue you find, cite the offending line(s) or snippet.\n",
    "\n",
    "A. Schema Mismatches  \n",
    "  - Columns or types that do not exist in the input layout but are referenced.  \n",
    "  - Missing required output columns or wrong data types for them.\n",
    "\n",
    "B. Unused or Redundant Elements  \n",
    "  - Columns, UDFs, caches, or joins that are created but never used downstream.\n",
    "\n",
    "C. Performance Risks  \n",
    "  - Wide shuffles, high-cardinality joins without broadcast/salt, `.collect()` on large DFs, `.repartition(1)`, etc.\n",
    "\n",
    "D. Anti-Patterns & Style  \n",
    "  - Hard-coded paths, secrets, magic numbers, long monolithic functions, non-PEP-8 names, replaceable UDFs.\n",
    "\n",
    "E. Security / Governance  \n",
    "  - Exposure of PII, unmasked secrets, non-encrypted S3/ADLS paths.\n",
    "\n",
    "Return the review in **markdown** with four sections:\n",
    "- **Critical Issues**   (must fix for correctness)  \n",
    "- **Performance Risks**  \n",
    "- **Style / Maintainability**  \n",
    "- **Quick Wins**\n",
    "\n",
    "=============================  TASK 2 - AUTO-REFACTOR  =============================\n",
    "Produce a **single, complete Python code block** that resolves every Critical Issue:\n",
    "\n",
    "* Align all source reads and writes with the provided layouts.  \n",
    "* Drop unused columns early; broadcast or cache judiciously.  \n",
    "* Replace UDFs with native Spark SQL functions when feasible.  \n",
    "* Parameterize paths/secrets; follow PEP-8 (`snake_case`, <= 79-char lines).  \n",
    "* Annotate each major step with a brief comment (what & why).  \n",
    "* Guarantee the final DataFrame exactly matches the **Output Layout**.\n",
    "\n",
    "Output format:\n",
    "```markdown\n",
    "### Review\n",
    "- ...\n",
    "\n",
    "### Refactored Code\n",
    "```python\n",
    "# full, runnable PySpark script with improvements\n",
    "# ...\n",
    "\n",
    "=============================  CONTEXT  SECTION  =============================\n",
    "## Input Layout (complete file)\n",
    "<input_layout>\n",
    "{input_layout_content}\n",
    "</input_layout>\n",
    "\n",
    "## Output Layout (complete file)\n",
    "<output_layout>\n",
    "{output_layout_content}\n",
    "</output_layout>\n",
    "\n",
    "## Additional Context (optional)\n",
    "<constraints>\n",
    "Running on Databricks Runtime 13.x, auto-scaling 2-8 nodes\n",
    "</constraints>\n",
    "\n",
    "## PySpark Code to Review\n",
    "<code>\n",
    "{output_case_study.choices[0].message.content}\n",
    "</code>\n",
    "----------------------------------------------------------------\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "code_review_python=f\"\"\"\n",
    "You are a principal Spark architect and code-review specialist.\n",
    "\n",
    "Your mission:\n",
    "\n",
    "1. **Analyse** the PySpark code against the supplied *Input Layout* and *Output Layout*.  \n",
    "   - Find schema mismatches, unused columns, performance risks, anti-patterns, and security issues.  \n",
    "   - Resolve every Critical Issue in the refactor.\n",
    "\n",
    "2. **Deliver exactly one thing**:  \n",
    "   **→ A single, complete PySpark code block** that incorporates all fixes and contains concise inline comments explaining *what was changed and why*.  \n",
    "   - No prose, no review section, no headings—just the final script inside triple back-ticks.  \n",
    "   - Conform to PEP-8 (`snake_case`, <= 79-char lines).  \n",
    "   - Align reads/writes with the provided layouts.  \n",
    "   - Drop unused columns early; use broadcast/caching judiciously.  \n",
    "   - Replace UDFs with native Spark SQL functions where possible.  \n",
    "   - Parameterise paths/secrets.  \n",
    "   - Ensure the resulting DataFrame exactly matches the **Output Layout**.\n",
    "\n",
    "Return format **must be only**:\n",
    "\n",
    "```python\n",
    "# refactored, runnable PySpark script with inline comments\n",
    "# ...\n",
    "\n",
    "=============================  CONTEXT  SECTION  =============================\n",
    "## Input Layout (complete file)\n",
    "<input_layout>\n",
    "{input_layout_content}\n",
    "</input_layout>\n",
    "\n",
    "## Output Layout (complete file)\n",
    "<output_layout>\n",
    "{output_layout_content}\n",
    "</output_layout>\n",
    "\n",
    "## Additional Context (optional)\n",
    "<constraints>\n",
    "Running on Databricks Runtime 13.x, auto-scaling 2-8 nodes\n",
    "</constraints>\n",
    "\n",
    "## PySpark Code to Review\n",
    "<code>\n",
    "{output_case_study.choices[0].message.content}\n",
    "</code>\n",
    "----------------------------------------------------------------\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "code_review=o3minicall(code_review_prompt,\"high\")\n",
    "print(code_review.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "code_review_python=o3minicall(code_review_python,\"high\")\n",
    "print(code_review_python.choices[0].message.content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv-agents",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
